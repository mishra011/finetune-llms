{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deepakmishra/.pyenv/versions/3.10.14/envs/denv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 7.47k/7.47k [00:00<00:00, 8.84MB/s]\n",
      "Downloading data: 100%|██████████| 24.2M/24.2M [00:02<00:00, 11.5MB/s]\n",
      "Generating train split: 100%|██████████| 52002/52002 [00:00<00:00, 748435.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\") \n",
    "train = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 53/53 [00:00<00:00, 93.37ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45835669"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_csv('train.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'falcontuned'\n",
    "model_name = 'tiiuae/falcon-7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_to_hub = False\n",
    "hf_token = \"hf_ixxxxxxxxxxxx\"\n",
    "repo_id = \"mishra011ai/myfalcon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "num_epochs = 4\n",
    "batch_size = 1\n",
    "block_size = 1024\n",
    "trainer = \"sft\"\n",
    "warmup_ratio = 0.1\n",
    "weight_decay = 0.01\n",
    "gradient_accumulation = 4\n",
    "use_fp16 = True\n",
    "use_peft = True\n",
    "use_int4 = True\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
    "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
    "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"USE_PEFT\"] = str(use_peft)\n",
    "os.environ[\"USE_INT4\"] = str(use_int4)\n",
    "os.environ[\"LORA_R\"] = str(lora_r)\n",
    "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
    "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-04 17:25:29\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-04 17:25:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: config, train, backend, func, version, deploy, inference\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-04 17:25:29\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-04 17:25:29\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'falcontuned/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-04 17:25:29\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'tiiuae/falcon-7b', 'project_name': 'falcontuned', 'data_path': 'data/', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 1024, 'model_max_length': 1024, 'padding': None, 'trainer': 'default', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 4, 'batch_size': 1, 'warmup_ratio': 0.1, 'gradient_accumulation': 4, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': False, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.045, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'text', 'rejected_text_column': 'rejected', 'push_to_hub': False, 'username': None, 'token': None, 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-04 17:25:36\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_default\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mStarting default/generic CLM training...\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-04 17:25:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 23, in train\n",
      "    train_default(config)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/train_clm_default.py\", line 29, in train\n",
      "    train_data, valid_data = utils.process_input_data(config)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/utils.py\", line 347, in process_input_data\n",
      "    train_data = load_dataset(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/load.py\", line 2587, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/load.py\", line 2259, in load_dataset_builder\n",
      "    dataset_module = dataset_module_factory(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/load.py\", line 1910, in dataset_module_factory\n",
      "    raise e1 from None\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/load.py\", line 1855, in dataset_module_factory\n",
      "    raise e\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/load.py\", line 1828, in dataset_module_factory\n",
      "    dataset_info = hf_api.dataset_info(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'data/'.\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-04 17:25:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mRepo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'data/'.\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-04 17:25:37\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 6932\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--epochs ${NUM_EPOCHS} \\\n",
    "--block-size ${BLOCK_SIZE} \\\n",
    "--warmup-ratio ${WARMUP_RATIO} \\\n",
    "--lora-r ${LORA_R} \\\n",
    "--lora-alpha ${LORA_ALPHA} \\\n",
    "--lora-dropout ${LORA_DROPOUT} \\\n",
    "--weight-decay ${WEIGHT_DECAY} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "#$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--fp16\" ) \\\n",
    "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
    "#$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--use-int4\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
