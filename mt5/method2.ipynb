{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:26\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-08 18:16:26\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: backend, deploy, version, inference, func, train, config\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:26\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:26\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'tinyproject/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:26\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'TinyLlama/TinyLlama_v1.1', 'project_name': 'tinyproject', 'data_path': 'argilla/distilabel-capybara-dpo-7k-binarized', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'orpo', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 1, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': 'chatml', 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'chosen', 'rejected_text_column': 'rejected', 'push_to_hub': True, 'username': 'mishra011ai', 'token': '*****', 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:32\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_orpo\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mStarting ORPO training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:41\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['source', 'conversation', 'original_response', 'generation_prompt', 'raw_generation_responses', 'new_generations', 'prompt', 'chosen', 'rejected', 'rating_chosen', 'rating_rejected', 'chosen_model', 'rejected_model'],\n",
      "    num_rows: 7563\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:41\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "tokenizer_config.json: 100%|███████████████████| 776/776 [00:00<00:00, 2.34MB/s]\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 4.48MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:01<00:00, 1.81MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 1.68MB/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:44\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_data_with_chat_template\u001b[0m:\u001b[36m446\u001b[0m - \u001b[1mApplying chat template\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:44\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_data_with_chat_template\u001b[0m:\u001b[36m447\u001b[0m - \u001b[1mFor ORPO/DPO, `prompt` will be extracted from chosen messages\u001b[0m\n",
      "Map: 100%|█████████████████████████| 7563/7563 [00:01<00:00, 4397.24 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "config.json: 100%|█████████████████████████████| 560/560 [00:00<00:00, 3.76MB/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-08 18:16:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m625\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m627\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:16:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "pytorch_model.bin: 100%|███████████████████| 4.40G/4.40G [02:31<00:00, 29.1MB/s]\n",
      "generation_config.json: 100%|███████████████████| 129/129 [00:00<00:00, 502kB/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:19:28\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mmodel dtype: torch.float32\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:19:30\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_orpo\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Map:   0%|                                      | 0/7563 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1565 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████████████████████| 7563/7563 [00:40<00:00, 185.30 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:20:13\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      "  0%|                                                  | 0/3782 [00:00<?, ?it/s]\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-08 18:20:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 43, in train\n",
      "    train_orpo(config)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/train_clm_orpo.py\", line 56, in train\n",
      "    trainer.train()\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/trainer.py\", line 1932, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/trainer.py\", line 2279, in _inner_training_loop\n",
      "    raise ValueError(\n",
      "ValueError: Calculated loss must be on the original device: cpu but device in use is mps:0\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-08 18:20:23\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mCalculated loss must be on the original device: cpu but device in use is mps:0\u001b[0m\n",
      "  0%|                                                  | 0/3782 [00:10<?, ?it/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 18:20:25\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 51138\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model TinyLlama/TinyLlama_v1.1 \\\n",
    "--data-path argilla/distilabel-capybara-dpo-7k-binarized \\\n",
    "--text-column chosen \\\n",
    "--rejected-text-column rejected \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 2 \\\n",
    "--epochs 1 \\\n",
    "--trainer orpo \\\n",
    "--chat-template chatml \\\n",
    "--peft \\\n",
    "--project-name tinyproject \\\n",
    "--username mishra011ai \\\n",
    "--push-to-hub \\\n",
    "--token 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m344\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: train, backend, config, deploy, inference, version, func\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m400\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'sftp-model/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:32\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1m{'model': 'microsoft/Phi-3-mini-4k-instruct', 'project_name': 'sftp-model', 'data_path': 'argilla/distilabel-capybara-dpo-7k-binarized', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': -1, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'eval_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': None, 'lr': 0.0002, 'epochs': 1, 'batch_size': 2, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.0, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': None, 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'prompt', 'text_column': 'chosen', 'rejected_text_column': 'rejected', 'push_to_hub': True, 'username': 'mishra011ai', 'token': '*****', 'unsloth': False}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:39\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:45\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m394\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['source', 'conversation', 'original_response', 'generation_prompt', 'raw_generation_responses', 'new_generations', 'prompt', 'chosen', 'rejected', 'rating_chosen', 'rating_rejected', 'chosen_model', 'rejected_model'],\n",
      "    num_rows: 7563\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:45\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m467\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m480\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:46\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m548\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m583\u001b[0m - \u001b[1mCan use unsloth: False\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m625\u001b[0m - \u001b[33m\u001b[1mUnsloth not available, continuing without it...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m627\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:43:47\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m635\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:19<00:00,  9.59s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:44:07\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mget_model\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mmodel dtype: torch.float32\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:44:11\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 0 examples [00:01, ? examples/s]\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-08 11:44:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m120\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1748, in _prepare_split_single\n",
      "    for key, record in generator:\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/packaged_modules/generator/generator.py\", line 30, in _generate_examples\n",
      "    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 620, in data_generator\n",
      "    yield from constant_length_iterator\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/utils.py\", line 512, in __iter__\n",
      "    tokenized_inputs = self.tokenizer(buffer, add_special_tokens=self.add_special_tokens, truncation=False)[\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2945, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 3004, in _call_one\n",
      "    raise ValueError(\n",
      "ValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 623, in _prepare_packed_dataloader\n",
      "    packed_dataset = Dataset.from_generator(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1125, in from_generator\n",
      "    ).read()\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/io/generator.py\", line 47, in read\n",
      "    self.builder.download_and_prepare(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1027, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1789, in _download_and_prepare\n",
      "    super()._download_and_prepare(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1122, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1627, in _prepare_split\n",
      "    for job_id, done, content in self._prepare_split_single(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/datasets/builder.py\", line 1784, in _prepare_split_single\n",
      "    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n",
      "datasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 28, in train\n",
      "    train_sft(config)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/autotrain/trainers/clm/train_clm_sft.py\", line 44, in train\n",
      "    trainer = SFTTrainer(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 362, in __init__\n",
      "    train_dataset = self._prepare_dataset(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 519, in _prepare_dataset\n",
      "    return self._prepare_packed_dataloader(\n",
      "  File \"/Users/deepakmishra/.pyenv/versions/denv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 627, in _prepare_packed_dataloader\n",
      "    raise ValueError(\n",
      "ValueError: Error occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[32m2024-07-08 11:44:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m121\u001b[0m - \u001b[31m\u001b[1mError occurred while packing the dataset. Make sure that your dataset has enough samples to at least yield one packed sequence.\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2024-07-08 11:44:15\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m350\u001b[0m - \u001b[1mJob ID: 48228\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model microsoft/Phi-3-mini-4k-instruct \\\n",
    "--data-path argilla/distilabel-capybara-dpo-7k-binarized \\\n",
    "--text-column chosen \\\n",
    "--rejected-text-column rejected \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 2 \\\n",
    "--epochs 1 \\\n",
    "--trainer sft \\\n",
    "--peft \\\n",
    "--project-name sftp-model \\\n",
    "--username mishra011ai \\\n",
    "--push-to-hub \\\n",
    "--token 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\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
